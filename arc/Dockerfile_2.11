FROM openjdk:8u242-jdk-slim-buster

# Versions
ARG ARC_VERSION
ARG SPARK_VERSION
ARG HADOOP_VERSION

# test to ensure all arguments have values then set environment variable
RUN test -n "$ARC_VERSION"
ENV ARC_VERSION $ARC_VERSION
RUN test -n "$SPARK_VERSION"
ENV SPARK_VERSION $SPARK_VERSION
RUN test -n "$HADOOP_VERSION"
ENV HADOOP_VERSION $HADOOP_VERSION

ENV SCALA_VERSION         2.11
ENV SPARK_HOME            /opt/spark
ENV SPARK_JARS            /opt/spark/jars
ENV HADOOP_HOME           /opt/hadoop
ENV SPARK_DOWNLOAD_URL    https://downloads.apache.org/spark
ENV SPARK_CHECKSUM_URL    https://www.apache.org/dist/spark
ENV SPARK_KEYS_URL        https://www.apache.org/dist/spark/KEYS
ENV HADOOP_DOWNLOAD_URL   https://downloads.apache.org/hadoop/common
ENV HADOOP_CHECKSUM_URL   https://www.apache.org/dist/hadoop/common
ENV HADOOP_KEYS_URL       https://www.apache.org/dist/hadoop/common/KEYS

RUN apt-get update && \
  apt-get upgrade -y -o Dir::Etc::SourceList=/etc/apt/security.sources.list && \
  apt-get install --no-install-recommends -y wget gpg gpg-agent && \
  rm -rf /var/lib/apt/lists/*

# add spark-without-hadoop
RUN mkdir -p ${SPARK_HOME} && \
  wget ${SPARK_DOWNLOAD_URL}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz && \
  wget ${SPARK_CHECKSUM_URL}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz.asc && \
  wget ${SPARK_KEYS_URL} && \
  gpg --import KEYS && \
  gpg --verify spark-${SPARK_VERSION}-bin-without-hadoop.tgz.asc && \
  gunzip -c spark-${SPARK_VERSION}-bin-without-hadoop.tgz | tar -xf - -C $SPARK_HOME --strip-components=1 && \
  rm -f spark-${SPARK_VERSION}-bin-without-hadoop.* && \
  rm KEYS

# add hadoop
RUN mkdir -p ${HADOOP_HOME} && \
  wget ${HADOOP_DOWNLOAD_URL}/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
  wget ${HADOOP_CHECKSUM_URL}/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz.asc && \
  wget ${HADOOP_KEYS_URL} && \
  gpg --import KEYS && \
  gpg --verify hadoop-${HADOOP_VERSION}.tar.gz.asc && \
  gunzip -c hadoop-${HADOOP_VERSION}.tar.gz | tar -xf - -C $HADOOP_HOME --strip-components=1 && \
  find /opt/hadoop -name "slf4j-log4j12-*.jar" | xargs rm && \
  find /opt/hadoop -name "guava-*.jar" | xargs rm && \
  rm -f hadoop*.tar.gz* && \
  rm KEYS

# link hadoop to spark as per https://spark.apache.org/docs/latest/hadoop-provided.html
RUN echo "export SPARK_DIST_CLASSPATH=\$(/opt/hadoop/bin/hadoop classpath)" >> ${SPARK_HOME}/conf/spark-env.sh

# get coursier to download the jars
# --force-version to force some older versions as some dependencies will break spark
# --exclude to remove libraires
# include any additional plugins at the bottom
RUN cd /tmp && \
  wget -P /tmp https://git.io/coursier-cli && \
  chmod +x /tmp/coursier-cli && \
  /tmp/coursier-cli fetch \
   --no-default \
  --repository ivy2Local \
  --repository central \
  --repository sonatype:snapshots \
  --repository https://repository.mulesoft.org/nexus/content/repositories/public \
  --force-version com.fasterxml.jackson.core:jackson-databind:2.6.7.1 \
  --force-version org.json4s:json4s-ast_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-core_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-jackson_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-scalap_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-scalap_${SCALA_VERSION}:3.5.3 \
  --force-version com.google.guava:guava:14.0.1 \
  --exclude org.slf4j:slf4j-nop \
  --exclude com.google.collections:google-collections \
  ai.tripl:arc_${SCALA_VERSION}:${ARC_VERSION} \
  ai.tripl:arc-cassandra-pipeline-plugin_${SCALA_VERSION}:1.2.0 \
  ai.tripl:arc-dataquality-udf-plugin_${SCALA_VERSION}:1.2.0 \
  ai.tripl:arc-deltalake-pipeline-plugin_${SCALA_VERSION}:1.6.0 \
  ai.tripl:arc-deltaperiod-config-plugin_${SCALA_VERSION}:1.1.0 \
  ai.tripl:arc-elasticsearch-pipeline-plugin_${SCALA_VERSION}:1.3.0 \
  ai.tripl:arc-kafka-pipeline-plugin_${SCALA_VERSION}:1.2.0 \
  ai.tripl:arc-mongodb-pipeline-plugin_${SCALA_VERSION}:1.1.0 \
  ai.tripl:arc-sas-pipeline-plugin_${SCALA_VERSION}:1.1.0 \
  com.facebook.presto:presto-jdbc:0.226 \
  com.microsoft.sqlserver:mssql-jdbc:7.4.1.jre8 \
  mysql:mysql-connector-java:8.0.17 \
  org.apache.hadoop:hadoop-aws:${HADOOP_VERSION} \
  org.apache.hadoop:hadoop-azure:${HADOOP_VERSION} \
  org.postgresql:postgresql:42.2.8 \
  com.oracle.ojdbc:ojdbc8:19.3.0.0

# move the jars to the spark/jars path for easy resolution
RUN find /root/.cache/coursier -name "*.jar" -print0 | xargs -0 -I {} mv {} ${SPARK_JARS} && \
  rm -rf /root/.cache  

# symlink the latest jar so spark-submit commands remain unchanged
RUN ln -s ${SPARK_JARS}/arc_${SCALA_VERSION}-${ARC_VERSION}.jar ${SPARK_JARS}/arc.jar

# copy in log4j.properties config file
COPY arc/log4j.properties ${SPARK_HOME}/conf/log4j.properties

# set workdir so bin/spark-submit works
WORKDIR ${SPARK_HOME}